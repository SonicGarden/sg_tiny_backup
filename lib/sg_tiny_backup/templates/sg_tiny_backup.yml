s3:
  db:
    # Destination url is `s3://{BUCKET}/{PREFIX}{TIMESTAMP}.sql.gz.enc`.
    bucket: YOUR_S3_BUCKET_NAME
    prefix: backup/database_
    access_key_id: <%= ENV.fetch('AWS_ACCESS_KEY_ID') %>
    secret_access_key: <%= ENV.fetch('AWS_SECRET_ACCESS_KEY') %>
    # If your backup file size exceeds 50GB, you must specify `expected_upload_size`.
    # It must be larger than your backup file size.
    # It is passed to `aws s3 cp`'s `--expected-size` option.
    # See https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html
    #
    # It is used for the calculation of S3 multipart upload chunk size not to exceed the part number limit.
    # See https://docs.aws.amazon.com/AmazonS3/latest/userguide/qfacts.html
    #
    # expected_upload_size: 100000000000
  log:
    # Destination url is `s3://{BUCKET}/{PREFIX}{TIMESTAMP}.tar.gz`.
    bucket: YOUR_S3_BUCKET_NAME
    prefix: backup/log_
    access_key_id: <%= ENV.fetch('AWS_ACCESS_KEY_ID') %>
    secret_access_key: <%= ENV.fetch('AWS_SECRET_ACCESS_KEY') %>
    # expected_upload_size: 100000000000
# You can generate ENCRYPTION_KEY with `bundle exec rails secret`.
encryption_key: <%= ENV.fetch('ENCRYPTION_KEY') %>
log:
  files:
    - log/production.log
    - log/production.log.1
pg_dump:
  extra_options: -xc --if-exists --encoding=utf8
# The following settings is not required since database config can be loaded from config/database.yml in typical rails application.
# db:
#   host: localhost
#   port: 5432
#   username: postgres
#   password: YOUR_DB_PASSWORD
